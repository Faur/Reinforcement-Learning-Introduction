{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Explain: Eligibility traces\n",
    "* Explain: lambda methods\n",
    "* Explain: True online SARSA lambda\n",
    "* Explain: Tree Backup lambda\n",
    "* code: Run and compare (on mountain car). Also compare with forwards view\n",
    "\n",
    "\n",
    "### DONE\n",
    "\n",
    "\n",
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility Traces for Reinforcement Learning\n",
    "\n",
    "We will now present another fundamental mechanism for Reinforcement Learning, the **eligibility trace**. These are another method that will allow us to generalize TD and Monte Carlo methods, just like n-step TD methods did, but in addition they are a more elegan and computationally efficient solution. **TODO: this intro sucks**\n",
    "\n",
    "An *eligibility trace* $\\textbf{z}_t$ acts as a **short-termed memory vector** that parallels the **long-term memory vector** represented by the weight vector $\\textbf{w}_t$. The eligibility trace \"keeps track\" of which components of $\\textbf{w}_t$ have contributed in producing the estimations. The \"memory\" stored in $\\textbf{z}_t$ begins then to *fade away* with a speed proportional to a choosen parameter $\\lambda$, and learning will then occour in the same component of $\\textbf{w}_t$ if a *non-zero TD error* (the new estimation lead to an improvement of the target)*occours before the trace fades back to zero*. **TODO: oh god this also sucks and is also super confusing**\n",
    "    \n",
    "The main advantages offered by eligibility traces over $n$-step TD methods is that we *only need to store the eligibility trace* and not *the whole past experience* (the last $n$ feature vectors). In addition, learning occours uniformly during the whole episode, withouth the need of a \"catching up\" when we reach termination. Finally, learning occours immediatly after the observations are taken, without the need of dealying it for $n$ timesteps.\n",
    "\n",
    "## Forward and Backward view\n",
    "\n",
    "\n",
    "## $\\lambda$-return\n",
    "### n-step truncated $\\lambda$-return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## True Online Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
