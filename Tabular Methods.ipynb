{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Explain: Q-learning\n",
    "* Explain: SARSA\n",
    "* Explain: n-step methods\n",
    "* Explain: cliff env.\n",
    "\n",
    "\n",
    "### DONE\n",
    "* code: Run and compare (on cliff)\n",
    "* Explain: tabular\n",
    "\n",
    "\n",
    "### NOTES\n",
    "* Example 6.6: Cliff Walking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to achieve goal-directed learning from interactions with an environment.\n",
    "At each time step, $t$, the agent recieves a state observation $S_t$ and a reward $R_t$, and performns an action $A_t$, like so:\n",
    "\n",
    "![Image of environment-agent interaction](img/env-agent.png)\n",
    "\n",
    "The agent must learn to pick actions that maximize the total expected future reward, called the **return**.\n",
    "\n",
    "For simple cases, where the actions and observation space are small and discrete it is possible to use tabular approaches, where each possible state-action pair is enumerated (in a table).\n",
    "Tabular approaches aren't applicable to real world problems, but are useful for understanding and inlluminating the fundamentals of reinforcement learning.\n",
    "\n",
    "This notebook describes tabular versions of two of the classical reinforcement learning algorithms: Q-learning and SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asdfs\n",
    "\n",
    "**TODO**: Value function approximation\n",
    "\n",
    "In practice we often use the discounted return instead of the actual return.\n",
    "Discounted returns weights imminent rewards higher than rewards in the far future.\n",
    "This is controlled by the discounting factor $\\gamma \\in [0, 1])$, like so:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^t \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "\n",
    "**TODO**: greedy actions\n",
    "\n",
    "**TODO**: epsilon greedy\n",
    "\n",
    "**TODO**: one step bootstrapping - n step boot strapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment: Cliff walker\n",
    "\n",
    "**TODO**: Describe environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from cliff import Cliff\n",
    "from agents import TabularNStepSARSA, TabularNStepQLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Below we train run tabular Q-learning and then SARSA agents.\n",
    "\n",
    "During training we monitor \n",
    " * the highest action value for each possible state (left plot), i.e. value of the greedy action of the agent.\n",
    " * the movement as heatmap (right plot) i.e. the number of times the agent has visited each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run settings\n",
    "num_runs = 10  # Number of runs to average rewards over\n",
    "eps_per_run = 500  # Number of episodes (terminations) per run\n",
    "n = 1  # n parameter in n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_QLearning = TabularNStepQLearning(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=eps_per_run)\n",
    "    TN_QLearning_rewards.append(rewards)\n",
    "\n",
    "TN_QLearning_rewards = np.array(TN_QLearning_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the last agent using visualizations.\n",
    "utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_SARSA_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    try:\n",
    "        TN_SARSA = TabularNStepSARSA(env.state_shape, env.num_actions, n=n)\n",
    "        _, rewards = utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=eps_per_run)\n",
    "        TN_SARSA_rewards.append(rewards)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "TN_SARSA_rewards = np.array(TN_SARSA_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the last agent using visualizations.\n",
    "utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n=1$ we see that the Q-learning agent learns the shortest path, right by the cliff, where as the SARSA agent learns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The code cell below plots the (smoothed) average reward for Q-learning and SARSA as a function of episodes.\n",
    "In the $n=1$ case we can clearly see that the risky 'shortes-path strategy' of the Q-learning agent doesn't payoff, and it generally recieves a lower reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "include_sd = False # include standard deviation in plot\n",
    "utils.reward_plotter(TN_QLearning_rewards, 'QLearning', 'r', include_sd=include_sd, smooth_factor=2)\n",
    "utils.reward_plotter(TN_SARSA_rewards, 'SARSA', 'b', include_sd=include_sd, smooth_factor=2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-100, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**TODO**: Describe: Why does Q learn the bad way?\n",
    "\n",
    "If we anneal the $\\epsilon$ to zero over time both agents will eventually learn to follow the safe path.\n",
    "\n",
    "Another way of helping the Q-learning agent is to increase the $n$ bootstrapping parameter, thus allowing it to account for longer consequences longer into the future.\n",
    "For any $n>1$ the \n",
    "\n",
    "\n",
    "**TODO**: Describe: re-run everything, but with $n=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run settings\n",
    "n = 5\n",
    "# We leave the other settings as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_QLearning = TabularNStepQLearning(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=eps_per_run)\n",
    "    TN_QLearning_rewards.append(rewards)\n",
    "TN_QLearning_rewards = np.array(TN_QLearning_rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_SARSA_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    try:\n",
    "        TN_SARSA = TabularNStepSARSA(env.state_shape, env.num_actions, n=n)\n",
    "        _, rewards = utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=eps_per_run)\n",
    "        TN_SARSA_rewards.append(rewards)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "TN_SARSA_rewards = np.array(TN_SARSA_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "include_sd = False # include standard deviation in plot\n",
    "utils.reward_plotter(TN_QLearning_rewards, 'QLearning', 'r', include_sd=include_sd, smooth_factor=2)\n",
    "utils.reward_plotter(TN_SARSA_rewards, 'SARSA', 'b', include_sd=include_sd, smooth_factor=2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-100, 0])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
