{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Explain: Q-learning\n",
    "* Explain: SARSA\n",
    "* Explain: n-step methods\n",
    "* Explain: cliff env.\n",
    "\n",
    "\n",
    "### DONE\n",
    "* code: Run and compare (on cliff)\n",
    "* Explain: tabular\n",
    "\n",
    "\n",
    "### NOTES\n",
    "* Example 6.6: Cliff Walking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of reinforcement learning is to achieve goal-directed learning from interactions with an environment.\n",
    "At each time step, $t$, the agent recieves a state observation $S_t$ and a reward $R_t$, and performns an action $A_t$, like so:\n",
    "\n",
    "![Image of environment-agent interaction](img/env-agent.png)\n",
    "\n",
    "The agent must learn to pick actions that maximize the total expected future reward, called the **return** $G_t$.\n",
    "In practice we often use the discounted return instead of the actual return.\n",
    "Discounted returns weights imminent rewards higher than rewards in the far future.\n",
    "This is controlled by the discounting factor $\\gamma \\in [0, 1])$, like so:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{k=0}^t \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "For simple cases, where the actions and observation space are small and discrete it is possible to use **tabular approaches**, where each possible state-action pair is enumerated (in a table).\n",
    "Tabular approaches aren't applicable to real world problems, but they can be useful for inlluminating the fundamental principles of reinforcement learning.\n",
    "\n",
    "This notebook describes tabular versions of two of the classical reinforcement learning algorithms: Q-learning and SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithms\n",
    "Most reinforcement learning mehtods involve estimating a **value function** â€” a function that estimate how good a given state or state-action pair is in terms of the expected return.\n",
    "The return of a state naturally depends on the agents behavior in the future, which is called the agents **policy**, value functions are therefore defined with respect to a policy.\n",
    "So the state-action pair, $(s,a)$, under behavior policy $\\pi$ is denoted as $q_\\pi(s,a)$.\n",
    "Similarly the state value function is $v_\\pi(s)$.\n",
    "In the tabular case the value function is represented as a table, with one entry for each unique state-action pair.\n",
    "The subsript is often omitted, as it is clear which policy is used.\n",
    "Value functions are nice to work with, as they automatically take into account what will happen in the future, greedily optimizing the value function is therefore equivalent to maximizing the longterm expected reward.\n",
    "\n",
    "Both Q-learning and SARSA estimate the state-action value function, and follow an **$\\epsilon$-greedy** policy.\n",
    "With probability $1-\\epsilon$ the action with the highest value is selected - $\\pi(s) = \\max_a q(s,a)$, and with probability a random action is selected.\n",
    "In this notebook we will use an $\\epsilon$-greedy policy, with $\\epsilon=0.1$ held constant, unless noted otherwise.\n",
    "Using a stochastic policy is desirable as it ensures that we explore the entire state space.\n",
    "$\\epsilon$-greedy is one of the simplest (and quite naive) solutions to the **exploration-exploitation** dillemma, but it works well for small to medium problems (e.g. it is sufficent for many, but not all Atari games).\n",
    "\n",
    "___\n",
    "\n",
    "One of the most important ideas witihin reinforcement learning is that of **temporal difference** (TD) methods.\n",
    "TD methods combine elements of dynamic programming and Monte Carlo methods (see Sutton and Barto [1] chapters 4 and 5), resulting in a methods that can solve environments with unknown dynamics in an online manner, i.e. by using existing estimates to perform the updates.\n",
    "\n",
    "**TODO**: Describe the ideas behind TD\n",
    "\n",
    "**TODO**: generalized policy iteration (GPI) - to solve the control problem\n",
    "\n",
    "## SARSA\n",
    "\n",
    "SARSA on policy TD-control method, meaning that the value function estimates the behavior policy.\n",
    "The value-function estimates are updated using the following equation:\n",
    "$$\n",
    "q(S_t, A_t) = q(S_t, A_t) + \\alpha \\big[ \n",
    "R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\n",
    "\\big]\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the step-size parameter, \n",
    "$q(S_t, A_t)$ is the current estimate, which we are updating, and \n",
    "$R_{t+1} + \\gamma q(S_{t+1}, A_{t+1}$ is the **target** of the update.\n",
    "For terminal state $S_t+1$ the value function q(S_{t+1}, A_{t+1}) is defined as zero.\n",
    "\n",
    "This algorithm is called **1-step tabular SARSA**, as the one true reward is used in the target, and the \n",
    "\n",
    "We are using an estimate () as the target of our update.\n",
    "\n",
    "## Q-learning\n",
    "Q-learrning update\n",
    "\n",
    "\n",
    "## What is the difference?\n",
    "\n",
    "\n",
    "**TODO**: TD Methods\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment: Cliff walker\n",
    "\n",
    "**TODO**: Describe environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import utils\n",
    "from cliff import Cliff\n",
    "from agents import TabularNStepSARSA, TabularNStepQLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Below we train run tabular Q-learning and then SARSA agents.\n",
    "\n",
    "During training we monitor \n",
    " * the highest action value for each possible state (left plot), i.e. value of the greedy action of the agent.\n",
    " * the movement as heatmap (right plot) i.e. the number of times the agent has visited each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Run settings\n",
    "num_runs = 10  # Number of runs to average rewards over\n",
    "eps_per_run = 500  # Number of episodes (terminations) per run\n",
    "n = 1  # n parameter in n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_QLearning = TabularNStepQLearning(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=eps_per_run)\n",
    "    TN_QLearning_rewards.append(rewards)\n",
    "\n",
    "TN_QLearning_rewards = np.array(TN_QLearning_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the last QLearning agent using visualizations.\n",
    "utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_SARSA_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    try:\n",
    "        TN_SARSA = TabularNStepSARSA(env.state_shape, env.num_actions, n=n)\n",
    "        _, rewards = utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=eps_per_run)\n",
    "        TN_SARSA_rewards.append(rewards)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "TN_SARSA_rewards = np.array(TN_SARSA_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the last SARSA agent using visualizations.\n",
    "utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $n=1$ we see that the Q-learning agent learns the shortest path, right by the cliff, where as the SARSA agent learns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The code cell below plots the (smoothed) average reward for Q-learning and SARSA as a function of episodes.\n",
    "In the $n=1$ case we can clearly see that the risky 'shortes-path strategy' of the Q-learning agent doesn't payoff, and it generally recieves a lower reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "include_sd = False # include standard deviation in plot\n",
    "utils.reward_plotter(TN_QLearning_rewards, 'QLearning', 'r', include_sd=include_sd, smooth_factor=2)\n",
    "utils.reward_plotter(TN_SARSA_rewards, 'SARSA', 'b', include_sd=include_sd, smooth_factor=2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-100, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we however change the epsilon to zero \n",
    "\n",
    "Q-learning is able to learn the underlying dynamics of the environemnt, and learns to approximate the value function for the optimal policy, not the policy it follows.\n",
    "\n",
    "(Since both the environment and the agents are both deterministic we only have to run one episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning.min_eps = 0\n",
    "TN_SARSA.min_eps = 0\n",
    "\n",
    "_, TN_QLearning_rewards_no_eps = utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=1, update=False)\n",
    "_, TN_SARSA_rewards_no_eps = utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=1, update=False)\n",
    "clear_output()\n",
    "\n",
    "print(\"Q-learning rewards with epsilon = 0:\", TN_QLearning_rewards_no_eps[0])\n",
    "print(\"SARSA rewards with epsilon = 0:     \", TN_SARSA_rewards_no_eps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-step bootstrapping\n",
    "\n",
    "**TODO**: - n step boot strapping\n",
    "\n",
    "**TODO**: Describe: Why does Q learn the bad way?\n",
    "\n",
    "If we anneal the $\\epsilon$ to zero over time both agents will eventually learn to follow the safe path.\n",
    "\n",
    "Another way of helping the Q-learning agent is to increase the $n$ bootstrapping parameter, thus allowing it to account for longer consequences longer into the future.\n",
    "For any $n>1$ the \n",
    "\n",
    "\n",
    "**TODO**: Describe: re-run everything, but with $n=5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run settings\n",
    "n = 5\n",
    "# We leave the other settings as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_QLearning = TabularNStepQLearning(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = utils.run_loop(env, TN_QLearning, 'QLearning, n='+str(n), max_e=eps_per_run)\n",
    "    TN_QLearning_rewards.append(rewards)\n",
    "TN_QLearning_rewards = np.array(TN_QLearning_rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_SARSA_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    try:\n",
    "        TN_SARSA = TabularNStepSARSA(env.state_shape, env.num_actions, n=n)\n",
    "        _, rewards = utils.run_loop(env, TN_SARSA, 'SARSA, n='+str(n), max_e=eps_per_run)\n",
    "        TN_SARSA_rewards.append(rewards)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "TN_SARSA_rewards = np.array(TN_SARSA_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "include_sd = False # include standard deviation in plot\n",
    "utils.reward_plotter(TN_QLearning_rewards, 'QLearning', 'r', include_sd=include_sd, smooth_factor=2)\n",
    "utils.reward_plotter(TN_SARSA_rewards, 'SARSA', 'b', include_sd=include_sd, smooth_factor=2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-100, 0])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographic Notes\n",
    "[1] Richard S. Sutton and Andrew G. Barto. 1998. Introduction to Reinforcement Learning (1st ed.). MIT Press, Cambridge, MA, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
