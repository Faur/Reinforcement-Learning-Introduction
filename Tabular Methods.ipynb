{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* Explain: tabular\n",
    "* Explain: Q-learning\n",
    "* Explain: SARSA\n",
    "* Explain: n-step methods\n",
    "* Explain: cliff env.\n",
    "\n",
    "\n",
    "### DONE\n",
    "* code: Run and compare (on cliff)\n",
    "\n",
    "\n",
    "### NOTES\n",
    "* Example 6.6: Cliff Walking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Methods for Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "from cliff import Cliff\n",
    "from agents import TabularNStepSARSA, TabularNStepQLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_loop(env, agent, title, max_e=None):\n",
    "    t = 0; i = 0; e = 0\n",
    "    s, r, d, _ = env.reset()\n",
    "    a_ = agent.action(s)\n",
    "    ep_lens = []; rewards = []\n",
    "    r_sum = 0\n",
    "    since_last_plot = 0\n",
    "\n",
    "    while True:\n",
    "        i += 1; t += 1; since_last_plot += 1\n",
    "        a = a_\n",
    "        s_, r, d, _ = env.step(a)\n",
    "        a_ = agent.action(s_)\n",
    "\n",
    "        agent.update(s=s, a=a, r=r, s_=s_, a_=a_, d=d)\n",
    "        r_sum += r\n",
    "        s = np.copy(s_)\n",
    "\n",
    "        if (e + 1) % 5000 == 0:\n",
    "            with utils.RunningPlot():\n",
    "                plt.figure(1, figsize=(4, 4))\n",
    "                clear_output(True)\n",
    "                plt.imshow(env.render())\n",
    "                plt.title('Episode:{}, step: {}'.format(e, i))\n",
    "\n",
    "        if d or i > 1e6:\n",
    "            if since_last_plot > 1e4:\n",
    "                with utils.RunningPlot():\n",
    "                    since_last_plot = 0\n",
    "                    clear_output(wait=True)\n",
    "                    plt.figure(1, figsize=(8, 4))\n",
    "                    plt.suptitle(title, x=0.1, y=1, fontsize=20, horizontalalignment='left')\n",
    "\n",
    "                    plt.subplot(121)\n",
    "                    plt.title('Highest action value')\n",
    "                    img1 = plt.imshow(np.max(agent.Qtable, -1))\n",
    "                    plt.axis('equal', frameon=True)\n",
    "                    utils.colorbar(img1)\n",
    "\n",
    "                    plt.subplot(122)\n",
    "                    plt.title('Movement Heatmap')\n",
    "                    img2 = plt.imshow(env.heat_map)\n",
    "                    plt.axis('equal')\n",
    "                    utils.colorbar(img2)\n",
    "\n",
    "            ep_lens.append(i)\n",
    "            rewards.append(r_sum)\n",
    "            r_sum = 0; e += 1; i = 0\n",
    "            s, r, d, _ = env.reset()\n",
    "\n",
    "        if max_e and e >= max_e:\n",
    "            break\n",
    "\n",
    "    return ep_lens, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "eps_per_run = 500\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_QLearning_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_QLearning = TabularNStepQLearning(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = run_loop(env, TN_QLearning, 'TabularNstepQLearning, n='+str(n), max_e=eps_per_run)\n",
    "    TN_QLearning_rewards.append(rewards)\n",
    "\n",
    "TN_QLearning_rewards = np.array(TN_QLearning_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN_SARSA_rewards = []\n",
    "env = Cliff()\n",
    "for i in range(num_runs):\n",
    "    TN_SARSA = TabularNStepSARSA(env.state_shape, env.num_actions, n=n)\n",
    "    _, rewards = run_loop(env, TN_SARSA, 'TabularNstepSARSA, n='+str(n), max_e=eps_per_run)\n",
    "    TN_SARSA_rewards.append(rewards)\n",
    "\n",
    "TN_SARSA_rewards = np.array(TN_SARSA_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "utils.reward_plotter(TN_SARSA_rewards, 'TabularNstepSARSA', 'b')\n",
    "utils.reward_plotter(TN_QLearning_rewards, 'TabularNstepQLearning', 'r')\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-100, 0])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
